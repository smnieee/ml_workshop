{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Regression\n",
    "Penalized Regression attempts to balance between bias and variance.\n",
    "\n",
    "Remember, for standard linear regression of a single variable, we use this equation:\n",
    "\n",
    "$\\bar{y} = m*x + b$\n",
    "\n",
    "However, if we want to make a more complex model, we can add more terms:\n",
    "\n",
    "$\\bar{y} = ... + \\beta_3*x^3 + \\beta_2*x^2 + \\beta_1*x + \\beta_0$\n",
    "\n",
    "\n",
    "Again, for standard linear regression of a single variable, the error is:\n",
    "\n",
    "$mse = \\frac{1}{N} \\sum{( y_i - \\bar{y}(x_i) )^2}$\n",
    "\n",
    "So, for more complex models, we have a lot more terms:\n",
    "\n",
    "$mse = \\frac{1}{N} \\sum{(y_i - (... + \\beta_3*x^3 + \\beta_2*x^2 + \\beta_1*x + \\beta_0))^2}$\n",
    "\n",
    "As we saw in the bias/variance tradeoff, we can get a more precise solution with a larger model, but two things happen:\n",
    "1. we lose generalization (overfitting)\n",
    "2. we have a lot of coefficients that get larger and larger.\n",
    "\n",
    "To address that, we really want to get rid of the some coefficients in order to make the model more robust.\n",
    "\n",
    "We can add a term to the error equation which penalizes coefficients:\n",
    "\n",
    "$pmse = \\frac{1}{N} \\sum{( y_i - \\bar{y}(x_i) )^2} + \\lambda \\sum_b{\\beta_b^2}$\n",
    "\n",
    "or:\n",
    "\n",
    "$pmse = \\frac{1}{N} \\sum{(y_i - (... + \\beta_3*x^3 + \\beta_2*x^2 + \\beta_1*x + \\beta_0))^2} + \\lambda \\sum_b{\\beta_b^2}$\n",
    "\n",
    "This says that large in the model increase the error.  As such, this regression attempts to keep all of the coefficients as small as possible.  It may also keep the coefficients well-balanced (essentially distributing the work across the different terms.  This is called __Ridge Regression__.  It does \"L2 Regularization\" (which is a term that you will hear, but not necessarily need to know).\n",
    "\n",
    "Another form of penalized regression attempts to force some coefficents towards zero (essentially throwing them out of the model).  This attempts to elliminate *\"unnecessary\"* coefficients and simplify the model.  This is called __LASSO Regression__. It does \"L1 Regularization\" (which is a term that you will hear, but not necessarily need to know).\n",
    "\n",
    "$pmse = \\frac{1}{N} \\sum{( y_i - \\bar{y}(x_i) )^2} + \\lambda \\sum_b{||\\beta_b||}$\n",
    "\n",
    "Lastly, you may want to have very few terms and have them be small.  This is __Elastic Net Regression__ which has both Ridge and Lasson built in:\n",
    "\n",
    "$pmse = \\frac{1}{N} \\sum{( y_i - \\bar{y}(x_i) )^2}  + \\lambda_1 \\sum_b{\\beta_b^2} + \\lambda_2 \\sum_b{||\\beta_b||}$\n",
    "\n",
    "\n",
    "__In both cases, we have a $\\lambda$ term__.  What is that for?  It is simply a constant that let's us set the relative influence of penalizing term.\n",
    "\n",
    "<font color='red'>\n",
    "## When would you want to use a large or small $\\lambda$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to look at using penalized regression in python.\n",
    "\n",
    "We are going to start with our previous data (boston housing prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=load_boston()\n",
    "boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "print(boston_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another column that contains the house prices which in scikit learn datasets are considered as target\n",
    "boston_df['Price']=boston.target\n",
    "print(boston_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newY=boston_df['Price']\n",
    "newX=boston_df.drop('Price',axis=1)\n",
    "print(newX[0:3]) # check \n",
    "print(\"Price\")\n",
    "print(newY[0:3]) # check \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing\n",
    "For the purpose of this analysis, we need two datasets.  The first is the **training dataset**.  We use this to build the model. The second is the **testing dataset** to test how well the model did. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Linear Regression to Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr = Ridge(alpha=0.1) \n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
    "rr100.fit(X_train, y_train)\n",
    "\n",
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "Ridge_train_score100 = rr100.score(X_train,y_train)\n",
    "Ridge_test_score100 = rr100.score(X_test, y_test)\n",
    "\n",
    "print(\"linear regression train score:\", train_score)\n",
    "print(\"linear regression test score:\", test_score)\n",
    "print(\"ridge regression train score low alpha:\", Ridge_train_score)\n",
    "print(\"ridge regression test score low alpha:\", Ridge_test_score)\n",
    "print(\"ridge regression train score high alpha:\", Ridge_train_score100)\n",
    "print(\"ridge regression test score high alpha:\", Ridge_test_score100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Linear Regression to LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "lar = Lasso(alpha=0.1) \n",
    "lar.fit(X_train, y_train)\n",
    "\n",
    "lar100 = Lasso(alpha=100) #  comparison with alpha value\n",
    "lar100.fit(X_train, y_train)\n",
    "\n",
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "\n",
    "Lasso_train_score = lar.score(X_train,y_train)\n",
    "Lasso_test_score = lar.score(X_test, y_test)\n",
    "Lasso_train_score100 = lar100.score(X_train,y_train)\n",
    "Lasso_test_score100 = lar100.score(X_test, y_test)\n",
    "\n",
    "lr_coeff_used = np.sum(lr.coef_!=0)\n",
    "coeff_used = np.sum(lar.coef_!=0)\n",
    "coeff_used_100 = np.sum(lar100.coef_!=0)\n",
    "\n",
    "print(\"linear regression train score:\", train_score)\n",
    "print(\"linear regression test score:\", test_score)\n",
    "print(\"Lasso regression train score low alpha:\", Lasso_train_score)\n",
    "print(\"Lasso regression test score low alpha:\", Lasso_test_score)\n",
    "print(\"Lasso regression train score high alpha:\", Lasso_train_score100)\n",
    "print(\"Lasso regression test score high alpha:\", Lasso_test_score100)\n",
    "\n",
    "print(\"Linear regression number of coefficients:\", lr_coeff_used)\n",
    "print(\"Lasso regression number of coefficients low alpha:\", coeff_used)\n",
    "print(\"Lasso regression number of coefficients high alpha:\", coeff_used_100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lar.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(lar100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 100$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for some read work to get done.\n",
    "We are going to use a whole new dataset and work out our own solutions.\n",
    "\n",
    "This is a breast cancer prediction dataset that looks at image features to try to predict breast cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "print(cancer_df.head(3))\n",
    "\n",
    "X = cancer.data\n",
    "Y = cancer.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y, test_size=0.3, random_state=31)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_train_score=lr.score(X_train,y_train)\n",
    "lr_test_score=lr.score(X_test,y_test)\n",
    "lr_coeff_used = np.sum(lr.coef_!=0)\n",
    "print(\"LR training score:\", lr_train_score)\n",
    "print( \"LR test score: \", lr_test_score)\n",
    "print(\"number of features used: \", lr_coeff_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to set your alpha!!!\n",
    "ridge = Ridge(alpha = _______)\n",
    "ridge.fit(X_train,y_train)\n",
    "train_score=ridge.score(X_train,y_train)\n",
    "test_score=ridge.score(X_test,y_test)\n",
    "ridge_coeff_used = np.sum(ridge.coef_!=0)\n",
    "print(\"training score:\", train_score) \n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", ridge_coeff_used)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(...\n",
    "              \n",
    "              \n",
    "              \n",
    "print(\"training score:\", train_score) \n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", lasso_coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic = ElasticNet(...\n",
    "                     \n",
    "                     \n",
    "print(\"training score:\", train_score) \n",
    "print(\"test score: \", test_score)\n",
    "print(\"number of features used: \", elastic_coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot your findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
