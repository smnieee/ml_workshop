{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tJ3ytIHTPr4"
      },
      "source": [
        "# ML Workshop Session 3: Advanced Topics in Supervised Learning\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "This machine learning workshop will build on our prior workshops. We will cover additional topics in supervised learning. From a design perspective, we will discuss splitting data into testing, training, and validation sets. This is critical to properly fitting the learning algorithms to the data. We will also discuss problems that may arise, such as over fitting to the data. Join us as we expand into practical considerations for implementation of supervised learning algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaEqbIEqAF0G"
      },
      "source": [
        "## Background\n",
        "---\n",
        "\n",
        "In the last workshop, we covered logistic regression and penalized regression. Part the that work included separating the data into a training set and a testing set. \n",
        "\n",
        "The training set was used to fit the hyperparameters of the model to the data. The testing set was then used to calculate the error in the model. This lead to a loop over epochs where the model fit was refined and the error remeasured until a sufficient fit was achieved.\n",
        "\n",
        "Now, we want to look at the iterative approach to fitting more rigorously. This includes another collection of data points, referred to as the validation set.\n",
        "\n",
        "The validation set will be used to assess how well our model predicts the results of new data being given to it.\n",
        "\n",
        "## Prior Workshops\n",
        "1. [Session 1]()\n",
        "2. [Session 2]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yV8lJWiiBp"
      },
      "source": [
        "## What's the Difference?\n",
        "\n",
        "The data that is being analyzed needs to serve multiple purposes. First, the data needs to be the foundation of the model we want to generate. It is representative of the relationships we want to capture in our model. Second, we need to check that our model fits inputs to outputs with minimal error. This error then determines if the model parameters (hyperparameters) need to be refined. Once the error is sufficiently small, a final check of the model is necessary. This final step creates a measurement for how the model handles _new_ data. These three steps represent training, validation, and testing respectively.\n",
        "\n",
        "Below is a picture of the division of available data into our three parts: train, validation, and test.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*Nv2NNALuokZEcV6hYEHdGA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2AZYsTs130G"
      },
      "source": [
        "## Why do we need multiple checks?\n",
        "\n",
        "![](https://www.lacan.upc.edu/admoreWeb/wp-content/uploads/2018/04/test2.jpg)\n",
        "\n",
        "A machine learning model is only useful if it can be used on future inputs not just data from the past. Obtaining a model that can be useful is difficult because of issues such as bias and variance.\n",
        "\n",
        "Bias is a difference between the underlying system's expected value and the model's expected value. Simplistically, it can be thought of as an offset between the true mean and the model's mean. If the model accurate represents the data, the bias should be very small. However, model setup and training can affect how large the bias is.\n",
        "\n",
        "Variance is a measure of how the data is spread around its mean. If our model fits the data well, the variance will be small. The variance will also look randomly distributed. We want to minimize variance but avoid overfitting the data to noise.\n",
        "\n",
        "Over fitting the data is difficult to avoid. We want our model to match the data as closely as possible. However, our data is just a sample of all possible data. If we fit the existing samples too closely we may not accurate represent all data. One way to watch for over fitting is to look at how the error in our model changes with the number of fitting iterations. Once the change in error begins to stabilize or asymptotically approach some value, continuing to fit the data with more iterations is likely starting to over fit to the data.\n",
        "\n",
        "By using the three divisions of our data, we can create a model that can be evaluated and refined to minimize bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhAT4OAp4t_X"
      },
      "source": [
        "## Great, so how do we split our data?\n",
        "\n",
        "The first thing to consider when creating your data splits is how much data is available. By considering the size of your data set, you can assess how much can be reasonable held back for the test set. For large collections, this can be 10-20% of your data.\n",
        "\n",
        "After reserving the test data, the remaining portion can be split into the training portion and the validation portion. Obviously, the more data used for training, the more closely your model will fit your data. However, without sufficient validation data, it will be difficult to get an accurate error estimate. Again, an estimate of 10-20% of your data can be used for validation.\n",
        "\n",
        "An extra aspect to discuss is cross-validation. If you have a limited dataset, it is more difficult to avoid over fitting the data. Cross-validation can be used to achieve better results than simply using a single validation set.\n",
        "\n",
        "In K-fold cross-validation, the data is split into $K$ bins, where $K << N$ with $N$ being the number of samples in the training-validation set. Then the training is run on $K-1$ bins, and the last bin is used for validation. This process is repeated for each permutation of training and validation assignments.The results of the fitting over all $K$ permuations are average to get an error estimate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrVpT2k4aJQ"
      },
      "source": [
        "# References\n",
        "1. [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
        "2. [What is the Difference Between Test and Validation Datasets?](https://machinelearningmastery.com/difference-test-validation-datasets/)\n",
        "\n"
      ]
    }
  ]
}