{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overfitting #1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfDkpcPk_BS"
      },
      "source": [
        "## **Overfitting, underfitting, bias-variance tradeoff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgdnzfmqlrlD"
      },
      "source": [
        "*This notebook is a collection of several articles online which describe the issue of overfitting and underfitting* [1,2]\n",
        "\n",
        "When developing a machine learning model, we are often excited to see great performance of our model.  \n",
        "\n",
        "Unfortunately, when a model performs well on training data, it is often an indication of poor performance in the real world.  Let's begin with a simple example of overfitting. \n",
        "\n",
        "[1] https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
        "\n",
        "[2] https://www.dataquest.io/blog/learning-curves-machine-learning/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJsZqaHbg-6C"
      },
      "source": [
        "# We've already heard about scikit-learn, the machine learning toolkit in python.  \n",
        "# In our previous workshops, we have used scikit-learn to build a regression model for some data.\n",
        "# The recipe will be the same; we will build a linear regression model, but it will be based on contrived data.\n",
        "\n",
        "# Begin by loading scikit-lean\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6l-q_uKnen6"
      },
      "source": [
        "# To demonstrate over fitting, we need a \"True\" function.  This is an sinusoid function that we are trying to fit a model to.\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "# Now we need to build our dataset.\n",
        "\n",
        "# seed() is used to ensure that we are all seeing the same plots.  You can change seed, if you want the data to come out differently.\n",
        "np.random.seed(0)\n",
        "\n",
        "# We are selecting 30 data points to fit to.\n",
        "n_samples = 30\n",
        "\n",
        "# We are going to choose some random points as input to the function\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "# Then we evaluate the function and we add some noise\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Samples from our Mystery function\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtMIP_ZMnvmv"
      },
      "source": [
        "# Given our samples of {(x0,y0),(x1,y1),(x2,y2),...,(xn,yn)}, let's fit a model to it.\n",
        "\n",
        "# Line model\n",
        "\n",
        "# A linear model of degree 1 is a line\n",
        "degree = 1\n",
        "polynomial_features = PolynomialFeatures(degree=degree,include_bias=False)\n",
        "\n",
        "# Linear Regression and the Pipeline will take care of the fitting, so we don't have to.  You can review the prior workshop for the math related to Linear Regression\n",
        "# https://www.youtube.com/watch?v=OG8ZFDBt5f0&t=4s\n",
        "linear_regression = LinearRegression()\n",
        "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "# This does the fit.  For any statistical model that you are developing, there will be a fit function.  It takes the X values (inputs) and the y values (known outputs).\n",
        "pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "# This \"scores\" the performance of the model\n",
        "scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "X_test = np.linspace(0, 1, 100)\n",
        "plt.scatter(X, pipeline.predict(X[:, np.newaxis]), label=\"Model\")\n",
        "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim((0, 1))\n",
        "plt.ylim((-2, 2))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Degree {}\\nMSE = {:.2}(+/- {:.2})\".format(degree, -scores.mean(), scores.std()))\n",
        "plt.show()\n",
        "\n",
        "# The Mean Square Error (MSE) is the difference between the known datapoints and the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYhWJ5CEsQ5p"
      },
      "source": [
        "# A line equation didn't do a very good job.  That is disappointing, but not surprising.  Maybe we could jump to a polynomial that is higher order\n",
        "\n",
        "# A polynomial model of degree 2\n",
        "degree = 2\n",
        "polynomial_features = PolynomialFeatures(degree=degree,include_bias=False)\n",
        "\n",
        "linear_regression = LinearRegression()\n",
        "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "# This does the fit.  For any statistical model that you are developing, there will be a fit function.  It takes the X values (inputs) and the y values (known outputs).\n",
        "pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "# This \"scores\" the performance of the model\n",
        "scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "X_test = np.linspace(0, 1, 100)\n",
        "plt.scatter(X, pipeline.predict(X[:, np.newaxis]), label=\"Model\")\n",
        "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim((0, 1))\n",
        "plt.ylim((-2, 2))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Degree {}\\nMSE = {:.2}(+/- {:.2})\".format(degree, -scores.mean(), scores.std()))\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTmre6Lkv2-K"
      },
      "source": [
        "# Degree 2 is better than Degree 1.  We should go to a higher order polynomial\n",
        "\n",
        "# INSERT YOU FAVORITE POLYNOMIAL DEGREE HERE\n",
        "myFavoritDegree = __\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZb7BsQguJVB"
      },
      "source": [
        "\n",
        "degree = myFavoritDegree\n",
        "polynomial_features = PolynomialFeatures(degree=degree,include_bias=False)\n",
        "\n",
        "linear_regression = LinearRegression()\n",
        "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "# This does the fit.  For any statistical model that you are developing, there will be a fit function.  It takes the X values (inputs) and the y values (known outputs).\n",
        "pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "# This \"scores\" the performance of the model\n",
        "scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "plt.scatter(X, pipeline.predict(X[:, np.newaxis]), label=\"Model\")\n",
        "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim((0, 1))\n",
        "plt.ylim((-2, 2))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Degree {}\\nMSE = {:.2}(+/- {:.2})\".format(degree, -scores.mean(), scores.std()))\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkr53MBjyZOi"
      },
      "source": [
        "# Remember that regression models build a curve, so that it can sample points other than the known samples.\n",
        "# Let's add some additional points into the mix and see how it does\n",
        "\n",
        "\n",
        "# ADD SOME NEW POINTS TO SEE WHAT HAPPENS\n",
        "newPoints = np.array([__,__,__,__])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UFlcoQJwHXP"
      },
      "source": [
        "\n",
        "\n",
        "degrees = [1, 2, myFavoritDegree]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    plt.scatter(X, pipeline.predict(X[:, np.newaxis]), label=\"Model\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.scatter(newPoints, pipeline.predict(newPoints[:, np.newaxis]), label=\"New Points\")    \n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\".format(degrees[i]))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZpjIgkjyriY"
      },
      "source": [
        "# Now, we need to figure out what is really going on, because our new points just don't seem to fit as well.\n",
        "\n",
        "#  Let's look at the regression model and understand what function it created\n",
        "\n",
        "\n",
        "degrees = [1, 2, myFavoritDegree]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\") \n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\".format(degrees[i]))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9khmEBszROK"
      },
      "source": [
        "### **ROOM TO TAKE NOTES AMD LESSONS LEARNED**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "--\n",
        "\n"
      ]
    }
  ]
}